<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Augmenting Human Intelligence With AI | Tenzin Wangdhen&#39;s Blog</title>
<meta name="keywords" content="gpt, anki, ai">
<meta name="description" content="SocrateGPT helps you learn topics better by asking you questions about it.">
<meta name="author" content="Tenzin Wangdhen">
<link rel="canonical" href="https://tesseract-blog.netlify.app/posts/augmenting-human-intelligence-with-ai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://tesseract-blog.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tesseract-blog.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tesseract-blog.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tesseract-blog.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://tesseract-blog.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Augmenting Human Intelligence With AI" />
<meta property="og:description" content="SocrateGPT helps you learn topics better by asking you questions about it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tesseract-blog.netlify.app/posts/augmenting-human-intelligence-with-ai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-07T17:27:13-08:00" />
<meta property="article:modified_time" content="2023-03-07T17:27:13-08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Augmenting Human Intelligence With AI"/>
<meta name="twitter:description" content="SocrateGPT helps you learn topics better by asking you questions about it."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tesseract-blog.netlify.app/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Augmenting Human Intelligence With AI",
      "item": "https://tesseract-blog.netlify.app/posts/augmenting-human-intelligence-with-ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Augmenting Human Intelligence With AI",
  "name": "Augmenting Human Intelligence With AI",
  "description": "SocrateGPT helps you learn topics better by asking you questions about it.",
  "keywords": [
    "gpt", "anki", "ai"
  ],
  "articleBody": "TLDR I created SocratesGPT to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.\nWhy? Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the Ebbinghaus forgetting curve. Memory is like a leaky bucket, and it was incredibly frustrating to realize I’d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.\nOne issue with Anki is that you can start to “overfit” on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you’ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I’ve found that most of my cards end up being “cloze deletions”, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.\nWhat if we could use AI to help us understand a topic better?\nSocratesGPT Unless you’ve been living under a particularly forlorn boulder, you’ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI’s API is gpt-3.5-turbo. This space is moving fast. When I started this project, the most advanced model was text-davinci-003 which was 10x more expensive and somewhat slower.\nGPT (generative pre-trained transformer) is a type of “large language model” (LLM) (sorry about the acronyms). By using “prompt engineering”, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app’s “backend” can consist of a single prompt.\nI saw a great example of this at https://github.com/varunshenoy/GraphGPT. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.\nI created SocratesGPT to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.\nIn my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.\nCreating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.\n“Prompt Engineering” Prompt engineering is process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.\nI started with a fairly simple prompt:\nYou are impersonating a question generator using the socratic method. You will be given a prompt, and you must respond with a question about that prompt. The response must be a valid json object with the following fields: prompt, question, answer, and correct. Eventually I had to add the following lines to get a cleaner response:\nYou cannot ask the same question twice. Try to limit the number of words in the choices to 4. If a choice is selected, it should be marked as selected: true. If a choice is not selected, it should be marked as selected: false. I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, you don’t want it to blow up the trolley. So you end up adding additional clauses: “err, also don’t blow it up!”.\nThis is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial data structure:\nExamples: current state: { \"counter\": 1, \"prompt\": \"\", \"questions\": [{}] } prompt is the text the model should generate questions on. questions is an array of question objects.\nThen I show an example prompt, and the expected response from the model:\nprompt: The sky is blue. new state: { \"counter\": 1, \"prompt\": \"The sky is blue.\", \"questions\": [{ \"question\": \"What color is the sky?\", \"choices\": [{ \"text\": \"blue\", \"correct\": true, \"selected\": false }, { \"text\": \"red\", \"correct\": false, \"selected\": false }, { \"text\": \"green\", \"correct\": false, \"selected\": false }, { \"text\": \"yellow\", \"correct\": false, \"selected\": false }] }] } Each question includes the generated question, along with an array of choices. The selected boolean is used in the front end to maintain the state of which options the user clicked, so these aren’t lost when a new prompt is generated.\nFinally we create a way for us to dynamically update the prompt:\ncurrent state: $state prompt: $prompt new state: In the front end, we inject the current state into $state and the prompt into $prompt:\nfetch(\"prompts/data.prompt\") .then((response) =\u003e response.text()) .then((text) =\u003e text.replace(\"$prompt\", prompt)) .then((text) =\u003e text.replace(\"$state\", JSON.stringify(state))) We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the temperature is set to 0.3. Temperature controls the “creativity” of the model. A higher temperature means the model is more likely to generate creative responses. A lower temperature means the model is more likely to generate responses that are more likely to be correct. frequency_penalty sets a penalty for repeating words. presence_penalty sets a penalty for repeating n-grams. top_p sets the probability mass function cutoff.\nconst DEFAULT_PARAMS = { model: \"gpt-3.5-turbo\", temperature: 0.3, max_tokens: 800, top_p: 1, frequency_penalty: 0, presence_penalty: 0, }; After the user clicks “Generate Question”, the value of $prompt is obtained from the prompt text area in the UI. The $state starts as a skeleton of the data structure provided in the training example. We call OpenAI’s new gpt-3.5-turbo chat completions API with data.prompt. The JSON response contained in data.choices[0].message.content is parsed to update the state and render the question in the UI.\nI found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There’s probably a cleaner way to do this. I also feed the entire prompt as the role user, but some parts of it may be better suited to the system role.\nconst params = { ...DEFAULT_PARAMS, messages: [{ role: \"user\", content: prompt }], }; ... const requestOptions = { method: \"POST\", headers: { \"Content-Type\": \"application/json\", Authorization: \"Bearer \" + String(apiKey || OPENAI_API_KEY), }, body: JSON.stringify(params), }; fetch(\"https://api.openai.com/v1/chat/completions\", requestOptions) .then((response) =\u003e response.json()) .then((data) =\u003e { const text = data.choices[0].message.content; // replace ' with \" to make it valid JSON const new_state = JSON.parse(text.replace(/'/g, '\"')); setState(new_state; If the user generates another question, we pass in the $state which now includes the first question. This lets the model we know what it already asked.\nLimitations There are several limitations with the current implementation:\nWhen providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is “The sky is blue”, the model may generate a question like “Why is the sky blue?” even though the reason is not provided in the text. This is related to the “grounding problem”, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations. Occasionally, the model seems to “undo” changes in state. For example, a question might get removed from state on the next run. It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times. Setting frequency_penalty and presense_penalty parameters to 1 seems to result in invalid JSON. This might be because they make the model less likely to generate characters like { multiple times, which is required for the JSON to be valid. Future There’s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be “fine-tuned” to ask more effective questions. You might even be able to implement reinforcement learning from human feedback (RLHF). Students could rate the quality of questions, which is then fed back into the model.\nAs multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from “hallucinating” questions that are not in the input text.\nThe “T” in GPT stands for transformer, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.\nAdvancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.\n",
  "wordCount" : "1728",
  "inLanguage": "en",
  "datePublished": "2023-03-07T17:27:13-08:00",
  "dateModified": "2023-03-07T17:27:13-08:00",
  "author":{
    "@type": "Person",
    "name": "Tenzin Wangdhen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tesseract-blog.netlify.app/posts/augmenting-human-intelligence-with-ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tenzin Wangdhen's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tesseract-blog.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tesseract-blog.netlify.app/" accesskey="h" title="Tenzin Wangdhen&#39;s Blog (Alt + H)">Tenzin Wangdhen&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tesseract-blog.netlify.app/">Home</a>&nbsp;»&nbsp;<a href="https://tesseract-blog.netlify.app/posts/">Posts</a></div>
    <h1 class="post-title">
      Augmenting Human Intelligence With AI
    </h1>
    <div class="post-description">
      SocrateGPT helps you learn topics better by asking you questions about it.
    </div>
    <div class="post-meta"><span title='2023-03-07 17:27:13 -0800 PST'>March 7, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1728 words&nbsp;·&nbsp;Tenzin Wangdhen&nbsp;|&nbsp;<a href="https://github.com/sinzin91/tesseract-blog/blob/master/content/posts/augmenting-human-intelligence-with-ai.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#tldr">TLDR</a></li>
    <li><a href="#why">Why?</a></li>
    <li><a href="#socratesgpt">SocratesGPT</a></li>
    <li><a href="#prompt-engineering">&ldquo;Prompt Engineering&rdquo;</a></li>
    <li><a href="#limitations">Limitations</a></li>
    <li><a href="#future">Future</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="tldr">TLDR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h2>
<p>I created <a href="https://github.com/sinzin91/socrates-gpt">SocratesGPT</a> to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.</p>
<p><img loading="lazy" src="/images/demo.gif" alt="SocratesGPT"  />
</p>
<h2 id="why">Why?<a hidden class="anchor" aria-hidden="true" href="#why">#</a></h2>
<p>Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the <a href="https://en.wikipedia.org/wiki/Forgetting_curve">Ebbinghaus forgetting curve</a>. Memory is like a leaky bucket, and it was incredibly frustrating to realize I&rsquo;d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.</p>
<p>One issue with Anki is that you can start to &ldquo;overfit&rdquo; on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you&rsquo;ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I&rsquo;ve found that most of my cards end up being &ldquo;cloze deletions&rdquo;, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.</p>
<p>What if we could use AI to help us understand a topic better?</p>
<h2 id="socratesgpt">SocratesGPT<a hidden class="anchor" aria-hidden="true" href="#socratesgpt">#</a></h2>
<p>Unless you&rsquo;ve been living under a particularly forlorn boulder, you&rsquo;ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI&rsquo;s API is <code>gpt-3.5-turbo</code>. This space is moving fast. When I started this project, the most advanced model was <code>text-davinci-003</code> which was 10x more expensive and somewhat slower.</p>
<p>GPT (generative pre-trained transformer) is a type of &ldquo;large language model&rdquo; (LLM) (sorry about the acronyms). By using &ldquo;prompt engineering&rdquo;, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app&rsquo;s &ldquo;backend&rdquo; can consist of a single prompt.</p>
<p>I saw a great example of this at <a href="https://github.com/varunshenoy/GraphGPT">https://github.com/varunshenoy/GraphGPT</a>. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.</p>
<p>I created <a href="https://github.com/sinzin91/socrates-gpt">SocratesGPT</a> to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.</p>
<p>In my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.</p>
<p>Creating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.</p>
<h2 id="prompt-engineering">&ldquo;Prompt Engineering&rdquo;<a hidden class="anchor" aria-hidden="true" href="#prompt-engineering">#</a></h2>
<p>Prompt engineering is process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.</p>
<p>I started with a fairly simple prompt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You are impersonating a question generator using the socratic method. 
</span></span><span style="display:flex;"><span>You will be given a prompt, and you must respond with a question about that prompt.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The response must be a valid json object with the following fields: 
</span></span><span style="display:flex;"><span>prompt, question, answer, and correct.
</span></span></code></pre></div><p>Eventually I had to add the following lines to get a cleaner response:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You cannot ask the same question twice. 
</span></span><span style="display:flex;"><span>Try to limit the number of words in the choices to 4. 
</span></span><span style="display:flex;"><span>If a choice is selected, it should be marked as selected: true. 
</span></span><span style="display:flex;"><span>If a choice is not selected, it should be marked as selected: false.
</span></span></code></pre></div><p>I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, you don&rsquo;t want it to blow up the trolley. So you end up adding additional clauses: &ldquo;err, also don&rsquo;t blow it up!&rdquo;.</p>
<p>This is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial  data structure:</p>
<pre tabindex="0"><code>Examples:

current state:
{
	&#34;counter&#34;: 1,
	&#34;prompt&#34;: &#34;&#34;,
	&#34;questions&#34;: [{}]
}
</code></pre><p><code>prompt</code> is the text the model should generate questions on. <code>questions</code> is an array of question objects.</p>
<p>Then I show an example prompt, and the expected response from the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>prompt: The sky is blue.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>new state:
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;counter&#34;</span>: <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;The sky is blue.&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;questions&#34;</span>: [{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;question&#34;</span>: <span style="color:#e6db74">&#34;What color is the sky?&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;choices&#34;</span>: [{
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;blue&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;correct&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;selected&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    }, {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;red&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;correct&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;selected&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    }, {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;green&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;correct&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;selected&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    }, {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;yellow&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;correct&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;selected&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    }]
</span></span><span style="display:flex;"><span>  }]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Each <code>question</code> includes the generated question, along with an array of <code>choices</code>. The <code>selected</code> boolean is used in the front end to maintain the state of which options the user clicked, so these aren&rsquo;t lost when a new prompt is generated.</p>
<p>Finally we create a way for us to dynamically update the prompt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>current state: $state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt: $prompt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>new state:
</span></span></code></pre></div><p>In the front end, we inject the current state into <code>$state</code> and the prompt into <code>$prompt</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#a6e22e">fetch</span>(<span style="color:#e6db74">&#34;prompts/data.prompt&#34;</span>)
</span></span><span style="display:flex;"><span>  .<span style="color:#a6e22e">then</span>((<span style="color:#a6e22e">response</span>) =&gt; <span style="color:#a6e22e">response</span>.<span style="color:#a6e22e">text</span>())
</span></span><span style="display:flex;"><span>  .<span style="color:#a6e22e">then</span>((<span style="color:#a6e22e">text</span>) =&gt; <span style="color:#a6e22e">text</span>.<span style="color:#a6e22e">replace</span>(<span style="color:#e6db74">&#34;$prompt&#34;</span>, <span style="color:#a6e22e">prompt</span>))
</span></span><span style="display:flex;"><span>  .<span style="color:#a6e22e">then</span>((<span style="color:#a6e22e">text</span>) =&gt; <span style="color:#a6e22e">text</span>.<span style="color:#a6e22e">replace</span>(<span style="color:#e6db74">&#34;$state&#34;</span>, <span style="color:#a6e22e">JSON</span>.<span style="color:#a6e22e">stringify</span>(<span style="color:#a6e22e">state</span>)))
</span></span></code></pre></div><p>We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the <code>temperature</code> is set to 0.3. Temperature controls the &ldquo;creativity&rdquo; of the model. A higher temperature means the model is more likely to generate creative responses. A lower temperature means the model is more likely to generate responses that are more likely to be correct. <code>frequency_penalty</code> sets a penalty for repeating words. <code>presence_penalty</code> sets a penalty for repeating n-grams. <code>top_p</code> sets the probability mass function cutoff.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">DEFAULT_PARAMS</span> <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">model</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">temperature</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">0.3</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">max_tokens</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">800</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">top_p</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">frequency_penalty</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">presence_penalty</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><p>After the user clicks &ldquo;Generate Question&rdquo;, the value of <code>$prompt</code> is obtained from the prompt text area in the UI. The <code>$state</code> starts as a skeleton of the data structure provided in the training example. We call OpenAI&rsquo;s new <code>gpt-3.5-turbo</code> chat completions API with <code>data.prompt</code>. The JSON response contained in  <code>data.choices[0].message.content</code> is parsed to update the state and render the question in the UI.</p>
<p>I found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There&rsquo;s probably a cleaner way to do this. I also feed the entire prompt as the role <code>user</code>, but some parts of it may be better suited to the <code>system</code> role.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">params</span> <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  ...<span style="color:#a6e22e">DEFAULT_PARAMS</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">messages</span><span style="color:#f92672">:</span> [{ <span style="color:#a6e22e">role</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#a6e22e">content</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">prompt</span> }],
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">requestOptions</span> <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">method</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;POST&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">headers</span><span style="color:#f92672">:</span> {
</span></span><span style="display:flex;"><span>	<span style="color:#e6db74">&#34;Content-Type&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;application/json&#34;</span>,
</span></span><span style="display:flex;"><span>	<span style="color:#a6e22e">Authorization</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;Bearer &#34;</span> <span style="color:#f92672">+</span> String(<span style="color:#a6e22e">apiKey</span> <span style="color:#f92672">||</span> <span style="color:#a6e22e">OPENAI_API_KEY</span>),
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">body</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">JSON</span>.<span style="color:#a6e22e">stringify</span>(<span style="color:#a6e22e">params</span>),
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">fetch</span>(<span style="color:#e6db74">&#34;https://api.openai.com/v1/chat/completions&#34;</span>, <span style="color:#a6e22e">requestOptions</span>)
</span></span><span style="display:flex;"><span>  .<span style="color:#a6e22e">then</span>((<span style="color:#a6e22e">response</span>) =&gt; <span style="color:#a6e22e">response</span>.<span style="color:#a6e22e">json</span>())
</span></span><span style="display:flex;"><span>  .<span style="color:#a6e22e">then</span>((<span style="color:#a6e22e">data</span>) =&gt; {
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">text</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">choices</span>[<span style="color:#ae81ff">0</span>].<span style="color:#a6e22e">message</span>.<span style="color:#a6e22e">content</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#75715e">// replace &#39; with &#34; to make it valid JSON
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>	<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">new_state</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">JSON</span>.<span style="color:#a6e22e">parse</span>(<span style="color:#a6e22e">text</span>.<span style="color:#a6e22e">replace</span>(<span style="color:#e6db74">/&#39;/g</span>, <span style="color:#e6db74">&#39;&#34;&#39;</span>));
</span></span><span style="display:flex;"><span>	<span style="color:#a6e22e">setState</span>(<span style="color:#a6e22e">new_state</span>;
</span></span></code></pre></div><p>If the user generates another question, we pass in the <code>$state</code> which now includes the first question. This lets the model we know what it already asked.</p>
<h2 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h2>
<p>There are several limitations with the current implementation:</p>
<ul>
<li>When providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is &ldquo;The sky is blue&rdquo;, the model may generate a question like &ldquo;Why is the sky blue?&rdquo; even though the reason is not provided in the text. This is related to the &ldquo;grounding problem&rdquo;, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations.</li>
<li>Occasionally, the model seems to &ldquo;undo&rdquo; changes in state. For example, a question might get removed from state on the next run.</li>
<li>It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times.</li>
<li>Setting <code>frequency_penalty</code> and <code>presense_penalty</code> parameters to <code>1</code> seems to result in invalid JSON. This might be because they make the model less likely to generate characters like <code>{</code> multiple times, which is required for the JSON to be valid.</li>
</ul>
<h2 id="future">Future<a hidden class="anchor" aria-hidden="true" href="#future">#</a></h2>
<p>There&rsquo;s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be &ldquo;fine-tuned&rdquo; to ask more effective questions. You might even be able to implement <a href="https://huggingface.co/blog/rlhf">reinforcement learning from human feedback (RLHF)</a>. Students could rate the quality of questions, which is then fed back into the model.</p>
<p>As multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from &ldquo;hallucinating&rdquo; questions that are not in the input text.</p>
<p>The &ldquo;T&rdquo; in GPT stands for <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a>, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.</p>
<p>Advancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tesseract-blog.netlify.app/tags/gpt/">gpt</a></li>
      <li><a href="https://tesseract-blog.netlify.app/tags/anki/">anki</a></li>
      <li><a href="https://tesseract-blog.netlify.app/tags/ai/">ai</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://tesseract-blog.netlify.app/">Tenzin Wangdhen&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
