<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Augmenting Human Intelligence With AI | Tenzin&#39;s Blog</title>
<meta name="keywords" content="gpt, anki, ai, project">
<meta name="description" content="SocratesGPT helps you learn topics better by asking you questions about it.">
<meta name="author" content="Tenzin Wangdhen">
<link rel="canonical" href="https://t12n.substack.com/p/augmenting-human-intelligence-with">
<meta name="google-site-verification" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e75e7bf7c4c8c239601c2bd3109564411a636eb048aa094f6a902bfe712e350a.css" integrity="sha256-515798TIwjlgHCvTEJVkQRpjbrBIqglPapAr/nEuNQo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1LK2EC2CML"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-1LK2EC2CML', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Augmenting Human Intelligence With AI" />
<meta property="og:description" content="SocratesGPT helps you learn topics better by asking you questions about it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/" />
<meta property="og:image" content="https://tenzinwangdhen.com/images/ai_socrates.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-07T17:27:13-08:00" />
<meta property="article:modified_time" content="2023-03-07T17:27:13-08:00" /><meta property="og:site_name" content="Tenzin&#39;s Blog" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://tenzinwangdhen.com/images/ai_socrates.png" />
<meta name="twitter:title" content="Augmenting Human Intelligence With AI"/>
<meta name="twitter:description" content="SocratesGPT helps you learn topics better by asking you questions about it."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://tenzinwangdhen.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Augmenting Human Intelligence With AI",
      "item": "https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Augmenting Human Intelligence With AI",
  "name": "Augmenting Human Intelligence With AI",
  "description": "SocratesGPT helps you learn topics better by asking you questions about it.",
  "keywords": [
    "gpt", "anki", "ai", "project"
  ],
  "articleBody": "TLDR I created SocratesGPT to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.\nWhy? Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the Ebbinghaus forgetting curve. Memory is like a leaky bucket, and it was incredibly frustrating to realize I’d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.\nOne issue with Anki is that you can start to “overfit” on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you’ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I’ve found that most of my cards end up being “cloze deletions”, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.\nWhat if we could use AI to help us understand a topic better?\nSocratesGPT Unless you’ve been living under a particularly forlorn boulder, you’ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI’s API is gpt-3.5-turbo. This space is moving fast. When I started this project, the most advanced model was text-davinci-003 which was 10x more expensive and somewhat slower. Edit: now GPT-4 is out.\nGPT (generative pre-trained transformer) is a type of “large language model” (LLM) (sorry about the acronyms). By using “prompt engineering”, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app’s “backend” can consist of a single prompt.\nI saw a great example of this at https://github.com/varunshenoy/GraphGPT. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.\nI created SocratesGPT to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.\nIn my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.\nCreating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.\n“Prompt Engineering” Prompt engineering is the process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.\nI started with a fairly simple prompt:\nYou are impersonating a question generator using the socratic method. You will be given a prompt, and you must respond with a question about that prompt. The response must be a valid json object with the following fields: prompt, question, answer, and correct. Eventually I had to add the following lines to get a cleaner response:\nYou cannot ask the same question twice. Try to limit the number of words in the choices to 4. If a choice is selected, it should be marked as selected: true. If a choice is not selected, it should be marked as selected: false. I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, the genie might find blowing up the trolley to be a perfectly reasonable solution. So you end up adding additional clauses: “err, also don’t blow it up!”.\nThis is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial data structure:\nExamples: current state: { \"counter\": 1, \"prompt\": \"\", \"questions\": [{}] } prompt is the text the model should generate questions on. questions is an array of question objects.\nThen I show an example prompt, and the expected response from the model:\nprompt: The sky is blue. new state: { \"counter\": 1, \"prompt\": \"The sky is blue.\", \"questions\": [{ \"question\": \"What color is the sky?\", \"choices\": [{ \"text\": \"blue\", \"correct\": true, \"selected\": false }, { \"text\": \"red\", \"correct\": false, \"selected\": false }, { \"text\": \"green\", \"correct\": false, \"selected\": false }, { \"text\": \"yellow\", \"correct\": false, \"selected\": false }] }] } Each question includes the generated question, along with an array of choices. The selected boolean is used in the front end to maintain the state of which options the user clicked, so these aren’t lost when a new prompt is generated.\nFinally we create a way for us to dynamically update the prompt:\ncurrent state: $state prompt: $prompt new state: In the front end, we inject the current state into $state and the prompt into $prompt:\nfetch(\"prompts/data.prompt\") .then((response) =\u003e response.text()) .then((text) =\u003e text.replace(\"$prompt\", prompt)) .then((text) =\u003e text.replace(\"$state\", JSON.stringify(state))) We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the temperature is set to 0.3. Temperature controls the “creativity” of the model. A lower temperature means the model is more likely to generate correct, almost deterministic responses. A higher temperature means the model generally leads to creative responses. frequency_penalty sets a penalty for repeating words. presence_penalty sets a penalty for repeating n-grams. top_p sets the probability mass function cutoff.\nconst DEFAULT_PARAMS = { model: \"gpt-3.5-turbo\", temperature: 0.3, max_tokens: 800, top_p: 1, frequency_penalty: 0, presence_penalty: 0, }; After the user clicks “Generate Question”, the value of $prompt is obtained from the prompt text area in the UI. The $state starts as a skeleton of the data structure provided in the training example. We call OpenAI’s new gpt-3.5-turbo chat completions API with data.prompt. The JSON response contained in data.choices[0].message.content is parsed to update the state and render the question in the UI.\nI found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There’s probably a cleaner way to do this. I also feed the entire prompt as the role user, but some parts of it may be better suited to the system role.\nconst params = { ...DEFAULT_PARAMS, messages: [{ role: \"user\", content: prompt }], }; ... const requestOptions = { method: \"POST\", headers: { \"Content-Type\": \"application/json\", Authorization: \"Bearer \" + String(apiKey || OPENAI_API_KEY), }, body: JSON.stringify(params), }; fetch(\"https://api.openai.com/v1/chat/completions\", requestOptions) .then((response) =\u003e response.json()) .then((data) =\u003e { const text = data.choices[0].message.content; // replace ' with \" to make it valid JSON const new_state = JSON.parse(text.replace(/'/g, '\"')); setState(new_state); If the user generates another question, we pass in the $state which now includes the first question. This lets the model we know what it already asked.\nLimitations There are several limitations with the current implementation:\nWhen providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is “The sky is blue”, the model may generate a question like “Why is the sky blue?” even though the reason is not provided in the text. This is related to the “grounding problem”, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations. Occasionally, the model seems to “undo” changes in state. For example, a question might get removed from state on the next run. It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times. Setting frequency_penalty and presense_penalty parameters to 1 seems to result in invalid JSON. This might be because they make the model less likely to generate characters like { multiple times, which is required for the JSON to be valid. Future There’s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be “fine-tuned” to ask more effective questions. You might even be able to implement reinforcement learning from human feedback (RLHF). Students could rate the quality of questions, which is then fed back into the model.\nAs multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from “hallucinating” questions that are not in the input text.\nThe “T” in GPT stands for transformer, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.\nAdvancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.\n",
  "wordCount" : "1733",
  "inLanguage": "en",
  "image":"https://tenzinwangdhen.com/images/ai_socrates.png","datePublished": "2023-03-07T17:27:13-08:00",
  "dateModified": "2023-03-07T17:27:13-08:00",
  "author":{
    "@type": "Person",
    "name": "Tenzin Wangdhen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tenzin's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tenzinwangdhen.com" accesskey="h" title="Tenzin&#39;s Blog (Alt + H)">
                <img src="https://tenzinwangdhen.com/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Tenzin&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tenzinwangdhen.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tenzinwangdhen.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tenzinwangdhen.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tenzinwangdhen.com">Home</a>&nbsp;»&nbsp;<a href="https://tenzinwangdhen.com/posts/">Posts</a></div>
    <h1 class="post-title">
      Augmenting Human Intelligence With AI
    </h1>
    <div class="post-description">
      SocratesGPT helps you learn topics better by asking you questions about it.
    </div>
    <div class="post-meta">&lt;span title=&#39;2023-03-07 17:27:13 -0800 PST&#39;&gt;March 7, 2023&lt;/span&gt;&amp;nbsp;·&amp;nbsp;9 min&amp;nbsp;·&amp;nbsp;1733 words&amp;nbsp;·&amp;nbsp;Tenzin Wangdhen&nbsp;|&nbsp;<a href="https://github.com/sinzin91/tesseract-blog/blob/master/content/posts/augmenting-human-intelligence-with-ai.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://tenzinwangdhen.com/images/ai_socrates.png" alt="Midjourney: artificial intelligence version of Socrates">
        <p>Midjourney: artificial intelligence version of Socrates</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#tldr">TLDR</a></li>
    <li><a href="#why">Why?</a></li>
    <li><a href="#socratesgpt">SocratesGPT</a></li>
    <li><a href="#prompt-engineering">&ldquo;Prompt Engineering&rdquo;</a></li>
    <li><a href="#limitations">Limitations</a></li>
    <li><a href="#future">Future</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="tldr">TLDR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h2>
<p>I created <a href="https://github.com/sinzin91/socrates-gpt">SocratesGPT</a> to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.</p>
<p><img loading="lazy" src="/images/socratesGPT_demo.gif" alt="SocratesGPT"  />
</p>
<h2 id="why">Why?<a hidden class="anchor" aria-hidden="true" href="#why">#</a></h2>
<p>Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the <a href="https://en.wikipedia.org/wiki/Forgetting_curve">Ebbinghaus forgetting curve</a>. Memory is like a leaky bucket, and it was incredibly frustrating to realize I&rsquo;d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.</p>
<p>One issue with Anki is that you can start to &ldquo;overfit&rdquo; on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you&rsquo;ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I&rsquo;ve found that most of my cards end up being &ldquo;cloze deletions&rdquo;, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.</p>
<p>What if we could use AI to help us understand a topic better?</p>
<h2 id="socratesgpt">SocratesGPT<a hidden class="anchor" aria-hidden="true" href="#socratesgpt">#</a></h2>
<p>Unless you&rsquo;ve been living under a particularly forlorn boulder, you&rsquo;ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI&rsquo;s API is <code>gpt-3.5-turbo</code>. This space is moving fast. When I started this project, the most advanced model was <code>text-davinci-003</code> which was 10x more expensive and somewhat slower. Edit: now GPT-4 is out.</p>
<p>GPT (generative pre-trained transformer) is a type of &ldquo;large language model&rdquo; (LLM) (sorry about the acronyms). By using &ldquo;prompt engineering&rdquo;, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app&rsquo;s &ldquo;backend&rdquo; can consist of a single prompt.</p>
<p>I saw a great example of this at <a href="https://github.com/varunshenoy/GraphGPT">https://github.com/varunshenoy/GraphGPT</a>. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.</p>
<p>I created <a href="https://github.com/sinzin91/socrates-gpt">SocratesGPT</a> to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.</p>
<p>In my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.</p>
<p>Creating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.</p>
<h2 id="prompt-engineering">&ldquo;Prompt Engineering&rdquo;<a hidden class="anchor" aria-hidden="true" href="#prompt-engineering">#</a></h2>
<p>Prompt engineering is the process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.</p>
<p>I started with a fairly simple prompt:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">You are impersonating a question generator using the socratic method. 
</span></span><span class="line"><span class="cl">You will be given a prompt, and you must respond with a question about that prompt.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">The response must be a valid json object with the following fields: 
</span></span><span class="line"><span class="cl">prompt, question, answer, and correct.
</span></span></code></pre></div><p>Eventually I had to add the following lines to get a cleaner response:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">You cannot ask the same question twice. 
</span></span><span class="line"><span class="cl">Try to limit the number of words in the choices to 4. 
</span></span><span class="line"><span class="cl">If a choice is selected, it should be marked as selected: true. 
</span></span><span class="line"><span class="cl">If a choice is not selected, it should be marked as selected: false.
</span></span></code></pre></div><p>I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, the genie might find blowing up the trolley to be a perfectly reasonable solution. So you end up adding additional clauses: &ldquo;err, also don&rsquo;t blow it up!&rdquo;.</p>
<p>This is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial  data structure:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Examples:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">current state:
</span></span><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">	&#34;counter&#34;: 1,
</span></span><span class="line"><span class="cl">	&#34;prompt&#34;: &#34;&#34;,
</span></span><span class="line"><span class="cl">	&#34;questions&#34;: [{}]
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></div><p><code>prompt</code> is the text the model should generate questions on. <code>questions</code> is an array of question objects.</p>
<p>Then I show an example prompt, and the expected response from the model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">prompt: The sky is blue.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">new state:
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;counter&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;prompt&#34;</span><span class="p">:</span> <span class="s2">&#34;The sky is blue.&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;questions&#34;</span><span class="p">:</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;question&#34;</span><span class="p">:</span> <span class="s2">&#34;What color is the sky?&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;choices&#34;</span><span class="p">:</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;text&#34;</span><span class="p">:</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;correct&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;selected&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;text&#34;</span><span class="p">:</span> <span class="s2">&#34;red&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;correct&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;selected&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;text&#34;</span><span class="p">:</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;correct&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;selected&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;text&#34;</span><span class="p">:</span> <span class="s2">&#34;yellow&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;correct&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;selected&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span>
</span></span><span class="line"><span class="cl">  <span class="p">}]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Each <code>question</code> includes the generated question, along with an array of <code>choices</code>. The <code>selected</code> boolean is used in the front end to maintain the state of which options the user clicked, so these aren&rsquo;t lost when a new prompt is generated.</p>
<p>Finally we create a way for us to dynamically update the prompt:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">current state: $state
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">prompt: $prompt
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">new state:
</span></span></code></pre></div><p>In the front end, we inject the current state into <code>$state</code> and the prompt into <code>$prompt</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="line"><span class="cl"><span class="nx">fetch</span><span class="p">(</span><span class="s2">&#34;prompts/data.prompt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">response</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="nx">response</span><span class="p">.</span><span class="nx">text</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">text</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="nx">text</span><span class="p">.</span><span class="nx">replace</span><span class="p">(</span><span class="s2">&#34;$prompt&#34;</span><span class="p">,</span> <span class="nx">prompt</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">text</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="nx">text</span><span class="p">.</span><span class="nx">replace</span><span class="p">(</span><span class="s2">&#34;$state&#34;</span><span class="p">,</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">state</span><span class="p">)))</span>
</span></span></code></pre></div><p>We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the <code>temperature</code> is set to 0.3. Temperature controls the &ldquo;creativity&rdquo; of the model. A lower temperature means the model is more likely to generate correct, almost deterministic responses. A higher temperature means the model generally leads to creative responses.  <code>frequency_penalty</code> sets a penalty for repeating words. <code>presence_penalty</code> sets a penalty for repeating n-grams. <code>top_p</code> sets the probability mass function cutoff.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="line"><span class="cl"><span class="kr">const</span> <span class="nx">DEFAULT_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nx">model</span><span class="o">:</span> <span class="s2">&#34;gpt-3.5-turbo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">temperature</span><span class="o">:</span> <span class="mf">0.3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">max_tokens</span><span class="o">:</span> <span class="mi">800</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">top_p</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">frequency_penalty</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">presence_penalty</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>After the user clicks &ldquo;Generate Question&rdquo;, the value of <code>$prompt</code> is obtained from the prompt text area in the UI. The <code>$state</code> starts as a skeleton of the data structure provided in the training example. We call OpenAI&rsquo;s new <code>gpt-3.5-turbo</code> chat completions API with <code>data.prompt</code>. The JSON response contained in  <code>data.choices[0].message.content</code> is parsed to update the state and render the question in the UI.</p>
<p>I found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There&rsquo;s probably a cleaner way to do this. I also feed the entire prompt as the role <code>user</code>, but some parts of it may be better suited to the <code>system</code> role.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="line"><span class="cl"><span class="kr">const</span> <span class="nx">params</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span><span class="nx">DEFAULT_PARAMS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">messages</span><span class="o">:</span> <span class="p">[{</span> <span class="nx">role</span><span class="o">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="nx">content</span><span class="o">:</span> <span class="nx">prompt</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kr">const</span> <span class="nx">requestOptions</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nx">method</span><span class="o">:</span> <span class="s2">&#34;POST&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nx">headers</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="s2">&#34;Content-Type&#34;</span><span class="o">:</span> <span class="s2">&#34;application/json&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nx">Authorization</span><span class="o">:</span> <span class="s2">&#34;Bearer &#34;</span> <span class="o">+</span> <span class="nb">String</span><span class="p">(</span><span class="nx">apiKey</span> <span class="o">||</span> <span class="nx">OPENAI_API_KEY</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="nx">body</span><span class="o">:</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">params</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nx">fetch</span><span class="p">(</span><span class="s2">&#34;https://api.openai.com/v1/chat/completions&#34;</span><span class="p">,</span> <span class="nx">requestOptions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">response</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="nx">response</span><span class="p">.</span><span class="nx">json</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">data</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kr">const</span> <span class="nx">text</span> <span class="o">=</span> <span class="nx">data</span><span class="p">.</span><span class="nx">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">message</span><span class="p">.</span><span class="nx">content</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// replace &#39; with &#34; to make it valid JSON
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kr">const</span> <span class="nx">new_state</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">text</span><span class="p">.</span><span class="nx">replace</span><span class="p">(</span><span class="sr">/&#39;/g</span><span class="p">,</span> <span class="s1">&#39;&#34;&#39;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">	<span class="nx">setState</span><span class="p">(</span><span class="nx">new_state</span><span class="p">);</span>
</span></span></code></pre></div><p>If the user generates another question, we pass in the <code>$state</code> which now includes the first question. This lets the model we know what it already asked.</p>
<h2 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h2>
<p>There are several limitations with the current implementation:</p>
<ul>
<li>When providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is &ldquo;The sky is blue&rdquo;, the model may generate a question like &ldquo;Why is the sky blue?&rdquo; even though the reason is not provided in the text. This is related to the &ldquo;grounding problem&rdquo;, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations.</li>
<li>Occasionally, the model seems to &ldquo;undo&rdquo; changes in state. For example, a question might get removed from state on the next run.</li>
<li>It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times.</li>
<li>Setting <code>frequency_penalty</code> and <code>presense_penalty</code> parameters to <code>1</code> seems to result in invalid JSON. This might be because they make the model less likely to generate characters like <code>{</code> multiple times, which is required for the JSON to be valid.</li>
</ul>
<h2 id="future">Future<a hidden class="anchor" aria-hidden="true" href="#future">#</a></h2>
<p>There&rsquo;s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be &ldquo;fine-tuned&rdquo; to ask more effective questions. You might even be able to implement <a href="https://huggingface.co/blog/rlhf">reinforcement learning from human feedback (RLHF)</a>. Students could rate the quality of questions, which is then fed back into the model.</p>
<p>As multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from &ldquo;hallucinating&rdquo; questions that are not in the input text.</p>
<p>The &ldquo;T&rdquo; in GPT stands for <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a>, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.</p>
<p>Advancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tenzinwangdhen.com/tags/gpt/">gpt</a></li>
      <li><a href="https://tenzinwangdhen.com/tags/anki/">anki</a></li>
      <li><a href="https://tenzinwangdhen.com/tags/ai/">ai</a></li>
      <li><a href="https://tenzinwangdhen.com/tags/project/">project</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://tenzinwangdhen.com/posts/book-review-peak/">
    <span class="title">« Prev</span>
    <br>
    <span>Book Review - Peak: Secrets from the New Science of Expertise</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on twitter"
        href="https://twitter.com/intent/tweet/?text=Augmenting%20Human%20Intelligence%20With%20AI&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&amp;hashtags=gpt%2canki%2cai%2cproject">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&amp;title=Augmenting%20Human%20Intelligence%20With%20AI&amp;summary=Augmenting%20Human%20Intelligence%20With%20AI&amp;source=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&title=Augmenting%20Human%20Intelligence%20With%20AI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on whatsapp"
        href="https://api.whatsapp.com/send?text=Augmenting%20Human%20Intelligence%20With%20AI%20-%20https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on telegram"
        href="https://telegram.me/share/url?text=Augmenting%20Human%20Intelligence%20With%20AI&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://tenzinwangdhen.com">Tenzin&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
