<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Augmenting Human Intelligence With AI | Tenzin's Blog</title><meta name=keywords content="gpt,anki,ai,project"><meta name=description content="SocratesGPT helps you learn topics better by asking you questions about it."><meta name=author content="Tenzin Wangdhen"><link rel=canonical href=https://t12n.substack.com/p/augmenting-human-intelligence-with><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.253e49683a95bbce0a3d8bc7080faa03f4e13d02880ee2cc370f949fb9d678f5.css integrity="sha256-JT5JaDqVu84KPYvHCA+qA/ThPQKIDuLMNw+Un7nWePU=" rel="preload stylesheet" as=style><link rel=icon href=https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-1LK2EC2CML"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1LK2EC2CML",{anonymize_ip:!1})}</script><meta property="og:title" content="Augmenting Human Intelligence With AI"><meta property="og:description" content="SocratesGPT helps you learn topics better by asking you questions about it."><meta property="og:type" content="article"><meta property="og:url" content="https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/"><meta property="og:image" content="https://tenzinwangdhen.com/images/ai_socrates.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-07T17:27:13-08:00"><meta property="article:modified_time" content="2023-03-07T17:27:13-08:00"><meta property="og:site_name" content="Tenzin's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tenzinwangdhen.com/images/ai_socrates.png"><meta name=twitter:title content="Augmenting Human Intelligence With AI"><meta name=twitter:description content="SocratesGPT helps you learn topics better by asking you questions about it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tenzinwangdhen.com/posts/"},{"@type":"ListItem","position":3,"name":"Augmenting Human Intelligence With AI","item":"https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Augmenting Human Intelligence With AI","name":"Augmenting Human Intelligence With AI","description":"SocratesGPT helps you learn topics better by asking you questions about it.","keywords":["gpt","anki","ai","project"],"articleBody":"TLDR I created SocratesGPT to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.\nWhy? Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the Ebbinghaus forgetting curve. Memory is like a leaky bucket, and it was incredibly frustrating to realize I’d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.\nOne issue with Anki is that you can start to “overfit” on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you’ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I’ve found that most of my cards end up being “cloze deletions”, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.\nWhat if we could use AI to help us understand a topic better?\nSocratesGPT Unless you’ve been living under a particularly forlorn boulder, you’ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI’s API is gpt-3.5-turbo. This space is moving fast. When I started this project, the most advanced model was text-davinci-003 which was 10x more expensive and somewhat slower. Edit: now GPT-4 is out.\nGPT (generative pre-trained transformer) is a type of “large language model” (LLM) (sorry about the acronyms). By using “prompt engineering”, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app’s “backend” can consist of a single prompt.\nI saw a great example of this at https://github.com/varunshenoy/GraphGPT. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.\nI created SocratesGPT to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.\nIn my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.\nCreating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.\n“Prompt Engineering” Prompt engineering is the process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.\nI started with a fairly simple prompt:\nYou are impersonating a question generator using the socratic method. You will be given a prompt, and you must respond with a question about that prompt. The response must be a valid json object with the following fields: prompt, question, answer, and correct. Eventually I had to add the following lines to get a cleaner response:\nYou cannot ask the same question twice. Try to limit the number of words in the choices to 4. If a choice is selected, it should be marked as selected: true. If a choice is not selected, it should be marked as selected: false. I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, the genie might find blowing up the trolley to be a perfectly reasonable solution. So you end up adding additional clauses: “err, also don’t blow it up!”.\nThis is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial data structure:\nExamples: current state: { \"counter\": 1, \"prompt\": \"\", \"questions\": [{}] } prompt is the text the model should generate questions on. questions is an array of question objects.\nThen I show an example prompt, and the expected response from the model:\nprompt: The sky is blue. new state: { \"counter\": 1, \"prompt\": \"The sky is blue.\", \"questions\": [{ \"question\": \"What color is the sky?\", \"choices\": [{ \"text\": \"blue\", \"correct\": true, \"selected\": false }, { \"text\": \"red\", \"correct\": false, \"selected\": false }, { \"text\": \"green\", \"correct\": false, \"selected\": false }, { \"text\": \"yellow\", \"correct\": false, \"selected\": false }] }] } Each question includes the generated question, along with an array of choices. The selected boolean is used in the front end to maintain the state of which options the user clicked, so these aren’t lost when a new prompt is generated.\nFinally we create a way for us to dynamically update the prompt:\ncurrent state: $state prompt: $prompt new state: In the front end, we inject the current state into $state and the prompt into $prompt:\nfetch(\"prompts/data.prompt\") .then((response) =\u003e response.text()) .then((text) =\u003e text.replace(\"$prompt\", prompt)) .then((text) =\u003e text.replace(\"$state\", JSON.stringify(state))) We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the temperature is set to 0.3. Temperature controls the “creativity” of the model. A lower temperature means the model is more likely to generate correct, almost deterministic responses. A higher temperature means the model generally leads to creative responses. frequency_penalty sets a penalty for repeating words. presence_penalty sets a penalty for repeating n-grams. top_p sets the probability mass function cutoff.\nconst DEFAULT_PARAMS = { model: \"gpt-3.5-turbo\", temperature: 0.3, max_tokens: 800, top_p: 1, frequency_penalty: 0, presence_penalty: 0, }; After the user clicks “Generate Question”, the value of $prompt is obtained from the prompt text area in the UI. The $state starts as a skeleton of the data structure provided in the training example. We call OpenAI’s new gpt-3.5-turbo chat completions API with data.prompt. The JSON response contained in data.choices[0].message.content is parsed to update the state and render the question in the UI.\nI found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There’s probably a cleaner way to do this. I also feed the entire prompt as the role user, but some parts of it may be better suited to the system role.\nconst params = { ...DEFAULT_PARAMS, messages: [{ role: \"user\", content: prompt }], }; ... const requestOptions = { method: \"POST\", headers: { \"Content-Type\": \"application/json\", Authorization: \"Bearer \" + String(apiKey || OPENAI_API_KEY), }, body: JSON.stringify(params), }; fetch(\"https://api.openai.com/v1/chat/completions\", requestOptions) .then((response) =\u003e response.json()) .then((data) =\u003e { const text = data.choices[0].message.content; // replace ' with \" to make it valid JSON const new_state = JSON.parse(text.replace(/'/g, '\"')); setState(new_state); If the user generates another question, we pass in the $state which now includes the first question. This lets the model we know what it already asked.\nLimitations There are several limitations with the current implementation:\nWhen providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is “The sky is blue”, the model may generate a question like “Why is the sky blue?” even though the reason is not provided in the text. This is related to the “grounding problem”, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations. Occasionally, the model seems to “undo” changes in state. For example, a question might get removed from state on the next run. It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times. Setting frequency_penalty and presense_penalty parameters to 1 seems to result in invalid JSON. This might be because they make the model less likely to generate characters like { multiple times, which is required for the JSON to be valid. Future There’s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be “fine-tuned” to ask more effective questions. You might even be able to implement reinforcement learning from human feedback (RLHF). Students could rate the quality of questions, which is then fed back into the model.\nAs multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from “hallucinating” questions that are not in the input text.\nThe “T” in GPT stands for transformer, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.\nAdvancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.\n","wordCount":"1733","inLanguage":"en","image":"https://tenzinwangdhen.com/images/ai_socrates.png","datePublished":"2023-03-07T17:27:13-08:00","dateModified":"2023-03-07T17:27:13-08:00","author":{"@type":"Person","name":"Tenzin Wangdhen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tenzinwangdhen.com/posts/augmenting-human-intelligence-with-ai/"},"publisher":{"@type":"Organization","name":"Tenzin's Blog","logo":{"@type":"ImageObject","url":"https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tenzinwangdhen.com accesskey=h title="Tenzin's Blog (Alt + H)"><img src=https://tenzinwangdhen.com/apple-touch-icon.png alt aria-label=logo height=35>Tenzin's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tenzinwangdhen.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tenzinwangdhen.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://tenzinwangdhen.com/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tenzinwangdhen.com>Home</a>&nbsp;»&nbsp;<a href=https://tenzinwangdhen.com/posts/>Posts</a></div><h1 class=post-title>Augmenting Human Intelligence With AI</h1><div class=post-description>SocratesGPT helps you learn topics better by asking you questions about it.</div><div class=post-meta><span title='2023-03-07 17:27:13 -0800 -0800'>March 7, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1733 words&nbsp;·&nbsp;Tenzin Wangdhen&nbsp;|&nbsp;<a href=https://github.com/sinzin91/tesseract-blog/blob/master/content/posts/augmenting-human-intelligence-with-ai.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src=https://tenzinwangdhen.com/images/ai_socrates.png alt="Midjourney: artificial intelligence version of Socrates"><p>Midjourney: artificial intelligence version of Socrates</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#tldr>TLDR</a></li><li><a href=#why>Why?</a></li><li><a href=#socratesgpt>SocratesGPT</a></li><li><a href=#prompt-engineering>&ldquo;Prompt Engineering&rdquo;</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#future>Future</a></li></ul></nav></div></details></div><div class=post-content><h2 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>I created <a href=https://github.com/sinzin91/socrates-gpt>SocratesGPT</a> to test the concept of using AI to generate questions about a given topic. Its goal you learn topics better by asking you questions about it. I also provide some technical details on the implementation.</p><p><img loading=lazy src=/images/socratesGPT_demo.gif alt=SocratesGPT></p><h2 id=why>Why?<a hidden class=anchor aria-hidden=true href=#why>#</a></h2><p>Over a year ago, I began using Anki, a spaced repetition flashcard app that helps me retain information over the long term by taking advantage of the <a href=https://en.wikipedia.org/wiki/Forgetting_curve>Ebbinghaus forgetting curve</a>. Memory is like a leaky bucket, and it was incredibly frustrating to realize I&rsquo;d forgotten most of what I learned. Anki has enabled me to confidently tackle difficult technical topics that I would have otherwise avoided.</p><p>One issue with Anki is that you can start to &ldquo;overfit&rdquo; on the questions you wrote. With Anki, you look at the card and then check the answer after trying to recall it. Once you&rsquo;ve seen the answer, you rate how easily you were able to recall it, which determines when the question will reappear in your deck. Creating questions can be time consuming, so I&rsquo;ve found that most of my cards end up being &ldquo;cloze deletions&rdquo;, i.e. fill-in-the-blank. This increases the risk of memorizing the answer to the card rather than truly understanding the concept.</p><p>What if we could use AI to help us understand a topic better?</p><h2 id=socratesgpt>SocratesGPT<a hidden class=anchor aria-hidden=true href=#socratesgpt>#</a></h2><p>Unless you&rsquo;ve been living under a particularly forlorn boulder, you&rsquo;ve heard about ChatGPT. You may not know that you can access GPT-3.5 as an API. The most advanced model available via OpenAI&rsquo;s API is <code>gpt-3.5-turbo</code>. This space is moving fast. When I started this project, the most advanced model was <code>text-davinci-003</code> which was 10x more expensive and somewhat slower. Edit: now GPT-4 is out.</p><p>GPT (generative pre-trained transformer) is a type of &ldquo;large language model&rdquo; (LLM) (sorry about the acronyms). By using &ldquo;prompt engineering&rdquo;, you can have the model return a structured JSON output for a given prompt, which can then be rendered in a UI. This means the app&rsquo;s &ldquo;backend&rdquo; can consist of a single prompt.</p><p>I saw a great example of this at <a href=https://github.com/varunshenoy/GraphGPT>https://github.com/varunshenoy/GraphGPT</a>. It shows an example of using one-shot prompting to teach GPT to give a JSON structured response. I was surprised to find that this style of prompting always gives back valid JSON, assuming you also set the model parameters appropriately.</p><p>I created <a href=https://github.com/sinzin91/socrates-gpt>SocratesGPT</a> to test the concept of using AI to generate questions about a given text. The Socratic method is a questioning style that encourages students to discover their beliefs and uncover hidden assumptions. However, SocratesGPT does not fully follow the Socratic style, as there is no real dialogue between student and teacher. The name is more aspirational than descriptive.</p><p>In my prompt, I ask the model to impersonate Socrates, and generate questions based on the given text. I also provide an example of the expected response in JSON format. The React front end parses the JSON and renders it. If you ask for another question, the app updates the prompt with the previous state of questions and options selected and pass it back to the model.</p><p>Creating a traditional backend to power an app like this without the use of LLMs would be highly non-trivial.</p><h2 id=prompt-engineering>&ldquo;Prompt Engineering&rdquo;<a hidden class=anchor aria-hidden=true href=#prompt-engineering>#</a></h2><p>Prompt engineering is the process of designing effective prompts for large language models such as GPT. Effective prompts are more likely to elicit high-quality outputs. Garbage prompt in, garbage response from the LLM.</p><p>I started with a fairly simple prompt:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>You are impersonating a question generator using the socratic method. 
</span></span><span class=line><span class=cl>You will be given a prompt, and you must respond with a question about that prompt.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>The response must be a valid json object with the following fields: 
</span></span><span class=line><span class=cl>prompt, question, answer, and correct.
</span></span></code></pre></div><p>Eventually I had to add the following lines to get a cleaner response:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>You cannot ask the same question twice. 
</span></span><span class=line><span class=cl>Try to limit the number of words in the choices to 4. 
</span></span><span class=line><span class=cl>If a choice is selected, it should be marked as selected: true. 
</span></span><span class=line><span class=cl>If a choice is not selected, it should be marked as selected: false.
</span></span></code></pre></div><p>I had the feeling of training an overly eager genie when phrasing my prompt. When you ask a genie to stop the trolley from hitting the pedestrian, the genie might find blowing up the trolley to be a perfectly reasonable solution. So you end up adding additional clauses: &ldquo;err, also don&rsquo;t blow it up!&rdquo;.</p><p>This is followed by a one-shot training example, where I provide a single example of what I want the model to do. I provide a starting state with my initial data structure:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Examples:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>current state:
</span></span><span class=line><span class=cl>{
</span></span><span class=line><span class=cl>	&#34;counter&#34;: 1,
</span></span><span class=line><span class=cl>	&#34;prompt&#34;: &#34;&#34;,
</span></span><span class=line><span class=cl>	&#34;questions&#34;: [{}]
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div><p><code>prompt</code> is the text the model should generate questions on. <code>questions</code> is an array of question objects.</p><p>Then I show an example prompt, and the expected response from the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>prompt: The sky is blue.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>new state:
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;counter&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;prompt&#34;</span><span class=p>:</span> <span class=s2>&#34;The sky is blue.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;questions&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;question&#34;</span><span class=p>:</span> <span class=s2>&#34;What color is the sky?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;choices&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;blue&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;correct&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;selected&#34;</span><span class=p>:</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;red&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;correct&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;selected&#34;</span><span class=p>:</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;green&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;correct&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;selected&#34;</span><span class=p>:</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;yellow&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;correct&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;selected&#34;</span><span class=p>:</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl>    <span class=p>}]</span>
</span></span><span class=line><span class=cl>  <span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Each <code>question</code> includes the generated question, along with an array of <code>choices</code>. The <code>selected</code> boolean is used in the front end to maintain the state of which options the user clicked, so these aren&rsquo;t lost when a new prompt is generated.</p><p>Finally we create a way for us to dynamically update the prompt:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>current state: $state
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>prompt: $prompt
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>new state:
</span></span></code></pre></div><p>In the front end, we inject the current state into <code>$state</code> and the prompt into <code>$prompt</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=nx>fetch</span><span class=p>(</span><span class=s2>&#34;prompts/data.prompt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>.</span><span class=nx>then</span><span class=p>((</span><span class=nx>response</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>response</span><span class=p>.</span><span class=nx>text</span><span class=p>())</span>
</span></span><span class=line><span class=cl>  <span class=p>.</span><span class=nx>then</span><span class=p>((</span><span class=nx>text</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>text</span><span class=p>.</span><span class=nx>replace</span><span class=p>(</span><span class=s2>&#34;$prompt&#34;</span><span class=p>,</span> <span class=nx>prompt</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=p>.</span><span class=nx>then</span><span class=p>((</span><span class=nx>text</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>text</span><span class=p>.</span><span class=nx>replace</span><span class=p>(</span><span class=s2>&#34;$state&#34;</span><span class=p>,</span> <span class=nx>JSON</span><span class=p>.</span><span class=nx>stringify</span><span class=p>(</span><span class=nx>state</span><span class=p>)))</span>
</span></span></code></pre></div><p>We have some control over the parameters of the model, such as the temperature, max tokens, and top p. I found that the model works best when the <code>temperature</code> is set to 0.3. Temperature controls the &ldquo;creativity&rdquo; of the model. A lower temperature means the model is more likely to generate correct, almost deterministic responses. A higher temperature means the model generally leads to creative responses. <code>frequency_penalty</code> sets a penalty for repeating words. <code>presence_penalty</code> sets a penalty for repeating n-grams. <code>top_p</code> sets the probability mass function cutoff.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>DEFAULT_PARAMS</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>model</span><span class=o>:</span> <span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>temperature</span><span class=o>:</span> <span class=mf>0.3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>max_tokens</span><span class=o>:</span> <span class=mi>800</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>top_p</span><span class=o>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>frequency_penalty</span><span class=o>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>presence_penalty</span><span class=o>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>After the user clicks &ldquo;Generate Question&rdquo;, the value of <code>$prompt</code> is obtained from the prompt text area in the UI. The <code>$state</code> starts as a skeleton of the data structure provided in the training example. We call OpenAI&rsquo;s new <code>gpt-3.5-turbo</code> chat completions API with <code>data.prompt</code>. The JSON response contained in <code>data.choices[0].message.content</code> is parsed to update the state and render the question in the UI.</p><p>I found that the prompt in the new API needs to use single quotes instead of quotes when sent to the model, and then I have to replace the single quotes in the response with double quotes again to make it valid JSON. There&rsquo;s probably a cleaner way to do this. I also feed the entire prompt as the role <code>user</code>, but some parts of it may be better suited to the <code>system</code> role.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>params</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span><span class=nx>DEFAULT_PARAMS</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>messages</span><span class=o>:</span> <span class=p>[{</span> <span class=nx>role</span><span class=o>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=nx>content</span><span class=o>:</span> <span class=nx>prompt</span> <span class=p>}],</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>requestOptions</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>method</span><span class=o>:</span> <span class=s2>&#34;POST&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>headers</span><span class=o>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;Content-Type&#34;</span><span class=o>:</span> <span class=s2>&#34;application/json&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=nx>Authorization</span><span class=o>:</span> <span class=s2>&#34;Bearer &#34;</span> <span class=o>+</span> <span class=nb>String</span><span class=p>(</span><span class=nx>apiKey</span> <span class=o>||</span> <span class=nx>OPENAI_API_KEY</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nx>body</span><span class=o>:</span> <span class=nx>JSON</span><span class=p>.</span><span class=nx>stringify</span><span class=p>(</span><span class=nx>params</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>fetch</span><span class=p>(</span><span class=s2>&#34;https://api.openai.com/v1/chat/completions&#34;</span><span class=p>,</span> <span class=nx>requestOptions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>.</span><span class=nx>then</span><span class=p>((</span><span class=nx>response</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>response</span><span class=p>.</span><span class=nx>json</span><span class=p>())</span>
</span></span><span class=line><span class=cl>  <span class=p>.</span><span class=nx>then</span><span class=p>((</span><span class=nx>data</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=kr>const</span> <span class=nx>text</span> <span class=o>=</span> <span class=nx>data</span><span class=p>.</span><span class=nx>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=nx>message</span><span class=p>.</span><span class=nx>content</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>// replace &#39; with &#34; to make it valid JSON
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kr>const</span> <span class=nx>new_state</span> <span class=o>=</span> <span class=nx>JSON</span><span class=p>.</span><span class=nx>parse</span><span class=p>(</span><span class=nx>text</span><span class=p>.</span><span class=nx>replace</span><span class=p>(</span><span class=sr>/&#39;/g</span><span class=p>,</span> <span class=s1>&#39;&#34;&#39;</span><span class=p>));</span>
</span></span><span class=line><span class=cl>	<span class=nx>setState</span><span class=p>(</span><span class=nx>new_state</span><span class=p>);</span>
</span></span></code></pre></div><p>If the user generates another question, we pass in the <code>$state</code> which now includes the first question. This lets the model we know what it already asked.</p><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><p>There are several limitations with the current implementation:</p><ul><li>When providing very short input text, the model may generate questions that are not grounded in the text. For example, if the input text is &ldquo;The sky is blue&rdquo;, the model may generate a question like &ldquo;Why is the sky blue?&rdquo; even though the reason is not provided in the text. This is related to the &ldquo;grounding problem&rdquo;, which is one of the major flaws of LLMs. Shorter input text provides less grounding, thus allowing for more hallucinations.</li><li>Occasionally, the model seems to &ldquo;undo&rdquo; changes in state. For example, a question might get removed from state on the next run.</li><li>It can take up to 20 seconds to receive a response from the API. The response time seems to increase with the size of the prompt. This can make it less engaging. Newer versions of the OpenAI API will likely have faster response times.</li><li>Setting <code>frequency_penalty</code> and <code>presense_penalty</code> parameters to <code>1</code> seems to result in invalid JSON. This might be because they make the model less likely to generate characters like <code>{</code> multiple times, which is required for the JSON to be valid.</li></ul><h2 id=future>Future<a hidden class=anchor aria-hidden=true href=#future>#</a></h2><p>There&rsquo;s a few ways SocratesGPT could be improved. Users could be able to save questions, and later be quizzed on them using spaced repetition. Larger context windows would allow the model to create questions on entire books. Eventually, GPT could be &ldquo;fine-tuned&rdquo; to ask more effective questions. You might even be able to implement <a href=https://huggingface.co/blog/rlhf>reinforcement learning from human feedback (RLHF)</a>. Students could rate the quality of questions, which is then fed back into the model.</p><p>As multi-modal models mature, the AI could even generate diagrams, such as an image of a section of the brain with an arrow pointing to the area that the user is asked to label. Finally, the grounding problem should be addressed to prevent the model from &ldquo;hallucinating&rdquo; questions that are not in the input text.</p><p>The &ldquo;T&rdquo; in GPT stands for <a href=https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)>transformer</a>, which is currently the most powerful deep learning architecture for several tasks, including NLP. There will likely be better architectures in the future. As the cost of GPU compute decreases, larger models with many more parameters become possible. By building on improvements like these, we can create ever more advanced AI tutors. These tutors will be able to understand your learning goals and current understanding to create personalized curriculums that are just difficult enough to challenge and ensure long term retention.</p><p>Advancing AI can help close the gap that MOOCs currently have difficulty filling, and help to scale education. Increased education leads to more people with advanced degrees that can help us solve some of our most pressing challenges. These are still the early days of LLMs.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tenzinwangdhen.com/tags/gpt/>gpt</a></li><li><a href=https://tenzinwangdhen.com/tags/anki/>anki</a></li><li><a href=https://tenzinwangdhen.com/tags/ai/>ai</a></li><li><a href=https://tenzinwangdhen.com/tags/project/>project</a></li></ul><nav class=paginav><a class=prev href=https://tenzinwangdhen.com/posts/book-review-peak/><span class=title>« Prev</span><br><span>Book Review - Peak: Secrets from the New Science of Expertise</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on twitter" href="https://twitter.com/intent/tweet/?text=Augmenting%20Human%20Intelligence%20With%20AI&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&amp;hashtags=gpt%2canki%2cai%2cproject"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&amp;title=Augmenting%20Human%20Intelligence%20With%20AI&amp;summary=Augmenting%20Human%20Intelligence%20With%20AI&amp;source=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f&title=Augmenting%20Human%20Intelligence%20With%20AI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on whatsapp" href="https://api.whatsapp.com/send?text=Augmenting%20Human%20Intelligence%20With%20AI%20-%20https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Augmenting Human Intelligence With AI on telegram" href="https://telegram.me/share/url?text=Augmenting%20Human%20Intelligence%20With%20AI&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2faugmenting-human-intelligence-with-ai%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tenzinwangdhen.com>Tenzin's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>