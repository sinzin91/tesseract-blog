<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Catastrophic Failure in Complex Systems | Tenzin&#39;s Blog</title>
<meta name="keywords" content="ai, automation, reliability">
<meta name="description" content="A look at the uncanny valley of system automation">
<meta name="author" content="Tenzin Wangdhen">
<link rel="canonical" href="https://tenzinwangdhen.com/posts/catastrophic-failure-complex-systems/">
<meta name="google-site-verification" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1443025edf561866ac87002e4fa1fcc308bee7d2cbae932ea8cd31d7383411f4.css" integrity="sha256-FEMCXt9WGGashwAuT6H8wwi&#43;59LLrpMuqM0x1zg0EfQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1LK2EC2CML"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-1LK2EC2CML', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Catastrophic Failure in Complex Systems" />
<meta property="og:description" content="A look at the uncanny valley of system automation" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tenzinwangdhen.com/posts/catastrophic-failure-complex-systems/" /><meta property="og:image" content="https://tenzinwangdhen.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-10T21:18:15-06:00" />
<meta property="article:modified_time" content="2024-12-10T21:18:15-06:00" /><meta property="og:site_name" content="Tenzin&#39;s Blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tenzinwangdhen.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Catastrophic Failure in Complex Systems"/>
<meta name="twitter:description" content="A look at the uncanny valley of system automation"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://tenzinwangdhen.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Catastrophic Failure in Complex Systems",
      "item": "https://tenzinwangdhen.com/posts/catastrophic-failure-complex-systems/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Catastrophic Failure in Complex Systems",
  "name": "Catastrophic Failure in Complex Systems",
  "description": "A look at the uncanny valley of system automation",
  "keywords": [
    "ai", "automation", "reliability"
  ],
  "articleBody": "Software systems exhibit a peculiar property: the more sophisticated they become, the more they tend toward catastrophic rather than graceful failure. This pattern isn’t unique to software - it’s characteristic of all complex systems, from neural networks to financial markets. But software’s rapid evolution and ubiquity makes it a particularly interesting case study.\nA complex system isn’t merely complicated. Rather, it possesses specific properties that make its behavior fundamentally unpredictable:\nDynamic interaction between components that can’t be reduced to simple cause-and-effect relationships Feedback loops that create non-linear responses to changes Emergent properties that can’t be predicted from analyzing individual components Nested complexity where components themselves are complex systems Consider a seemingly straightforward example: running a persistent database on Kubernetes. At first glance, this might appear to be just a collection of software components working together. But examine it more closely:\nThe database must maintain consistency across distributed nodes Kubernetes continuously adjusts resource allocation based on load Network latency creates feedback loops affecting query performance Storage systems interact with hardware in non-linear ways Each layer (database, Kubernetes, cloud infrastructure) is itself a complex system The result isn’t just a complicated stack of technology - it’s a system where changes in one component can propagate in unexpected ways, creating emergent behaviors that weren’t designed or anticipated.\nRichard Cook’s “How Complex Systems Fail” makes a counterintuitive claim: complex systems are always running in a partially broken state. This isn’t a flaw in implementation, but rather an inherent property of complexity itself. Why?\nThe number of potential interactions between components grows factorially with system size Each interaction represents a potential failure mode Testing all possible states becomes computationally intractable Individual “minor” flaws are too numerous to fully eliminate The system continues functioning despite these flaws due to human adaptation This leads to what we might call the “complexity paradox”: as systems grow more sophisticated in an attempt to prevent failures, they become more complex, which in turn makes them more prone to catastrophic failure modes.\nThe pattern becomes clearer when we examine how defenses against failure evolve:\nTechnical defenses (redundancy, monitoring, automated recovery) Organizational defenses (procedures, certifications, audits) Human defenses (training, expertise, tacit knowledge) Each layer of defense adds complexity, creating new potential failure modes even as it guards against known ones. The result is a system that appears more robust to anticipated problems while becoming more vulnerable to “black swan” events - rare but catastrophic failures that emerge from unexpected interactions between components.\nWhen catastrophic failures occur in complex systems, organizations typically respond with a search for the “root cause” - a fundamentally flawed approach that misunderstands the nature of complex system failures. These failures aren’t linear chains of causation but rather emergent phenomena arising from multiple interacting components and conditions. What makes a failure “catastrophic” rather than routine is not merely its impact, but its emergence from the subtle interplay between system components, creating cascade effects that overwhelm our carefully constructed defenses.\nThe impossibility of truly isolating root causes becomes clear when we consider the nature of complex systems: they are constantly evolving, operating with multiple simultaneous flaws, maintained by changing combinations of human and technical components, and subject to different environmental conditions at different times. The very notion of “cause” in such systems may be more a human construct than a meaningful description of system behavior.\nA particularly insidious aspect of complex system failures is what we might call the “hindsight fallacy” - the tendency to view past failures as obviously predictable once we know their outcome. This creates a dangerous illusion of preventability that drives misguided remediation efforts.\nConsider a Kubernetes cluster where application pods suddenly lose database connectivity due to an underscaled load balancer. Post-incident, the solution appears obvious: “We should have scaled the load balancer.” But this apparently simple insight obscures the reality of operating complex systems. The actual system state before failure was far more ambiguous, with multiple potential failure points existing simultaneously. Resources and attention were finite and had to be allocated across many concerns. The relationship between load balancer scaling and system stability wasn’t necessarily clear in advance.\nThe natural response to such failures is often automation - an approach that introduces what Cook and Parameswaran independently identify as a profound paradox. By attempting to prevent specific failure modes through automation, we paradoxically increase system complexity, create new potential failure modes, reduce operator engagement with the system, and mask accumulating problems until catastrophic failure occurs.\nTake the load balancer example: Implementing autoscaling seems like an obvious solution, but it introduces new complexities in configuration, documentation, monitoring requirements, and potential failure modes. Perhaps most critically, it reduces operator familiarity with manual scaling procedures. Over time, operators interact with the system less frequently, their skills atrophy, and the system becomes progressively less legible to human understanding.\nThe paradox of automation extends beyond merely increasing system complexity. In “The Control Revolution and Its Discontents”, Ashwin Parameswaran identifies a more subtle and pernicious effect: automation’s apparent safety creates an environment where human error can accumulate invisibly. By smoothing over minor issues and handling routine failures, automated systems mask the early warning signs that would traditionally alert operators to deteriorating performance.\nThis leads to what Parameswaran calls the “uncanny valley” of automation - a state where the system appears more reliable in normal operation, but operators become progressively deskilled and the potential for catastrophic failure actually increases. The system becomes a black box, operating in ways that are increasingly opaque to the humans nominally in charge of maintaining it.\nAn instructive analogy is that of self-driving cars. Early automotive automation - cruise control, lane assistance - augments human capability without introducing significant new risks. A sweet spot emerges where the machine handles routine tasks while the human driver remains engaged and capable, hands on the wheel, ready to respond to situations requiring judgment. The uncanny valley begins when automation extends into critical scenarios, fostering dangerous overconfidence in drivers who have grown complacent yet must still intervene in emergencies. Only true level 5 autonomy, with its complete elimination of uncertainty, would justify removing the human driver entirely.\nParameswaran’s analysis of the Air France Flight 447 crash in 2009 provides a haunting illustration of the uncanny valley in automation. In his essay “People Make Poor Monitors for Computers”, he shows how the automated systems that were meant to make the flight safer ultimately contributed to its catastrophic failure. The pilots, accustomed to the plane’s sophisticated autopilot handling most situations, found themselves suddenly forced to take manual control in challenging conditions. Their skills, dulled by routine reliance on automation, proved inadequate for the crisis they faced.\nWhat lies beyond the uncanny valley of automation? In principle, a state of perfect algorithmization where radical uncertainty has been eliminated entirely. This represents the ultimate goal of the “control revolution” - that centuries-long project to solve every problem through data and algorithms. In such a world, omniscient AI systems would handle all complexity, and human operators could take a much needed sabbatical.\nHow should we approach automation in complex systems, knowing that perfect algorithmic control remains aspirational? Parameswaran suggests a nuanced strategy: embrace automation for non-catastrophic failures while maintaining redundancy where failures could prove ruinous. This keeps human operators meaningfully engaged while benefiting from automated assistance. The goal is not to eliminate human involvement but to structure it properly - maintaining the feedback loops that build and preserve operator capability.\nCook arrives at similar conclusions through different reasoning. He argues that safety emerges from operators’ intimate familiarity with failure modes and system boundaries. Effective monitoring systems should help operators recognize the “edge of the envelope” - that threshold beyond which system behavior becomes unpredictable. This requires not just data collection but careful design of human-machine interfaces that make system state and trajectories legible to operators.\nThe fundamental insight is that safety isn’t a product to be purchased or a feature to be coded - it’s an emergent property arising from moment-to-moment adaptation by skilled human operators working with automated systems. Until we achieve true algorithmic omniscience (if ever), our focus must be on maintaining this creative tension between human and machine capabilities, keeping operators firmly within the loop while leveraging automation to extend rather than replace their capabilities.\n",
  "wordCount" : "1372",
  "inLanguage": "en",
  "datePublished": "2024-12-10T21:18:15-06:00",
  "dateModified": "2024-12-10T21:18:15-06:00",
  "author":{
    "@type": "Person",
    "name": "Tenzin Wangdhen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tenzinwangdhen.com/posts/catastrophic-failure-complex-systems/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tenzin's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tenzinwangdhen.com/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tenzinwangdhen.com" accesskey="h" title="Tenzin&#39;s Blog (Alt + H)">
                <img src="https://tenzinwangdhen.com/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Tenzin&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tenzinwangdhen.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tenzinwangdhen.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tenzinwangdhen.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tenzinwangdhen.com">Home</a>&nbsp;»&nbsp;<a href="https://tenzinwangdhen.com/posts/">Posts</a></div>
    <h1 class="post-title">
      Catastrophic Failure in Complex Systems
    </h1>
    <div class="post-description">
      A look at the uncanny valley of system automation
    </div>
    <div class="post-meta"><span title='2024-12-10 21:18:15 -0600 CST'>December 10, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1372 words&nbsp;·&nbsp;Tenzin Wangdhen&nbsp;|&nbsp;<a href="https://github.com/sinzin91/tesseract-blog/blob/master/content/posts/catastrophic-failure-complex-systems.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 

  <div class="post-content"><p>Software systems exhibit a peculiar property: the more sophisticated they become, the more they tend toward catastrophic rather than graceful failure. This pattern isn&rsquo;t unique to software - it&rsquo;s characteristic of all complex systems, from neural networks to financial markets. But software&rsquo;s rapid evolution and ubiquity makes it a particularly interesting case study.</p>
<p>A complex system isn&rsquo;t merely complicated. Rather, it possesses specific properties that make its behavior fundamentally unpredictable:</p>
<ol>
<li><strong>Dynamic interaction</strong> between components that can&rsquo;t be reduced to simple cause-and-effect relationships</li>
<li><strong>Feedback loops</strong> that create non-linear responses to changes</li>
<li><strong>Emergent properties</strong> that can&rsquo;t be predicted from analyzing individual components</li>
<li><strong>Nested complexity</strong> where components themselves are complex systems</li>
</ol>
<p>Consider a seemingly straightforward example: running a persistent database on Kubernetes. At first glance, this might appear to be just a collection of software components working together. But examine it more closely:</p>
<ul>
<li>The database must maintain consistency across distributed nodes</li>
<li>Kubernetes continuously adjusts resource allocation based on load</li>
<li>Network latency creates feedback loops affecting query performance</li>
<li>Storage systems interact with hardware in non-linear ways</li>
<li>Each layer (database, Kubernetes, cloud infrastructure) is itself a complex system</li>
</ul>
<p>The result isn&rsquo;t just a complicated stack of technology - it&rsquo;s a system where changes in one component can propagate in unexpected ways, creating emergent behaviors that weren&rsquo;t designed or anticipated.</p>
<p>Richard Cook&rsquo;s &ldquo;<a href="https://how.complexsystems.fail">How Complex Systems Fail</a>&rdquo; makes a counterintuitive claim: complex systems are <em>always</em> running in a partially broken state. This isn&rsquo;t a flaw in implementation, but rather an inherent property of complexity itself. Why?</p>
<ol>
<li>The number of potential interactions between components grows factorially with system size</li>
<li>Each interaction represents a potential failure mode</li>
<li>Testing all possible states becomes computationally intractable</li>
<li>Individual &ldquo;minor&rdquo; flaws are too numerous to fully eliminate</li>
<li>The system continues functioning despite these flaws due to human adaptation</li>
</ol>
<p>This leads to what we might call the &ldquo;complexity paradox&rdquo;: as systems grow more sophisticated in an attempt to prevent failures, they become more complex, which in turn makes them more prone to catastrophic failure modes.</p>
<p>The pattern becomes clearer when we examine how defenses against failure evolve:</p>
<ul>
<li>Technical defenses (redundancy, monitoring, automated recovery)</li>
<li>Organizational defenses (procedures, certifications, audits)</li>
<li>Human defenses (training, expertise, tacit knowledge)</li>
</ul>
<p>Each layer of defense adds complexity, creating new potential failure modes even as it guards against known ones. The result is a system that appears more robust to anticipated problems while becoming more vulnerable to &ldquo;black swan&rdquo; events - rare but catastrophic failures that emerge from unexpected interactions between components.</p>
<p>When catastrophic failures occur in complex systems, organizations typically respond with a search for the &ldquo;root cause&rdquo; - a fundamentally flawed approach that misunderstands the nature of complex system failures. These failures aren&rsquo;t linear chains of causation but rather emergent phenomena arising from multiple interacting components and conditions. What makes a failure &ldquo;catastrophic&rdquo; rather than routine is not merely its impact, but its emergence from the subtle interplay between system components, creating cascade effects that overwhelm our carefully constructed defenses.</p>
<p>The impossibility of truly isolating root causes becomes clear when we consider the nature of complex systems: they are constantly evolving, operating with multiple simultaneous flaws, maintained by changing combinations of human and technical components, and subject to different environmental conditions at different times. The very notion of &ldquo;cause&rdquo; in such systems may be more a human construct than a meaningful description of system behavior.</p>
<p>A particularly insidious aspect of complex system failures is what we might call the &ldquo;hindsight fallacy&rdquo; - the tendency to view past failures as obviously predictable once we know their outcome. This creates a dangerous illusion of preventability that drives misguided remediation efforts.</p>
<p>Consider a Kubernetes cluster where application pods suddenly lose database connectivity due to an underscaled load balancer. Post-incident, the solution appears obvious: &ldquo;We should have scaled the load balancer.&rdquo; But this apparently simple insight obscures the reality of operating complex systems. The actual system state before failure was far more ambiguous, with multiple potential failure points existing simultaneously. Resources and attention were finite and had to be allocated across many concerns. The relationship between load balancer scaling and system stability wasn&rsquo;t necessarily clear in advance.</p>
<p>The natural response to such failures is often automation - an approach that introduces what Cook and Parameswaran independently identify as a profound paradox. By attempting to prevent specific failure modes through automation, we paradoxically increase system complexity, create new potential failure modes, reduce operator engagement with the system, and mask accumulating problems until catastrophic failure occurs.</p>
<p>Take the load balancer example: Implementing autoscaling seems like an obvious solution, but it introduces new complexities in configuration, documentation, monitoring requirements, and potential failure modes. Perhaps most critically, it reduces operator familiarity with manual scaling procedures. Over time, operators interact with the system less frequently, their skills atrophy, and the system becomes progressively less legible to human understanding.</p>
<p>The paradox of automation extends beyond merely increasing system complexity. In &ldquo;<a href="https://macroresilience.com/2012/02/21/the-control-revolution-and-its-discontents-the-uncanny-valley/">The Control Revolution and Its Discontents</a>&rdquo;, Ashwin Parameswaran identifies a more subtle and pernicious effect: automation&rsquo;s apparent safety creates an environment where human error can accumulate invisibly. By smoothing over minor issues and handling routine failures, automated systems mask the early warning signs that would traditionally alert operators to deteriorating performance.</p>
<p>This leads to what Parameswaran calls the &ldquo;uncanny valley&rdquo; of automation - a state where the system appears more reliable in normal operation, but operators become progressively deskilled and the potential for catastrophic failure actually increases. The system becomes a black box, operating in ways that are increasingly opaque to the humans nominally in charge of maintaining it.</p>
<p><img loading="lazy" src="/images/uncanny-valley.png" alt="Uncanny Valley"  />
</p>
<p>An instructive analogy is that of self-driving cars. Early automotive automation - cruise control, lane assistance - augments human capability without introducing significant new risks. A sweet spot emerges where the machine handles routine tasks while the human driver remains engaged and capable, hands on the wheel, ready to respond to situations requiring judgment. The uncanny valley begins when automation extends into critical scenarios, fostering dangerous overconfidence in drivers who have grown complacent yet must still intervene in emergencies. Only true level 5 autonomy, with its complete elimination of uncertainty, would justify removing the human driver entirely.</p>
<p>Parameswaran&rsquo;s analysis of the Air France Flight 447 crash in 2009 provides a haunting illustration of the uncanny valley in automation. In his essay &ldquo;<a href="https://macroresilience.com/2011/12/29/people-make-poor-monitors-for-computers/">People Make Poor Monitors for Computers</a>&rdquo;, he shows how the automated systems that were meant to make the flight safer ultimately contributed to its catastrophic failure. The pilots, accustomed to the plane&rsquo;s sophisticated autopilot handling most situations, found themselves suddenly forced to take manual control in challenging conditions. Their skills, dulled by routine reliance on automation, proved inadequate for the crisis they faced.</p>
<p>What lies beyond the uncanny valley of automation? In principle, a state of perfect algorithmization where radical uncertainty has been eliminated entirely. This represents the ultimate goal of the &ldquo;control revolution&rdquo; - that centuries-long project to solve every problem through data and algorithms. In such a world, omniscient AI systems would handle all complexity, and human operators could take a much needed sabbatical.</p>
<p>How should we approach automation in complex systems, knowing that perfect algorithmic control remains aspirational? Parameswaran suggests a nuanced strategy: embrace automation for non-catastrophic failures while maintaining redundancy where failures could prove ruinous. This keeps human operators meaningfully engaged while benefiting from automated assistance. The goal is not to eliminate human involvement but to structure it properly - maintaining the feedback loops that build and preserve operator capability.</p>
<p>Cook arrives at similar conclusions through different reasoning. He argues that safety emerges from operators&rsquo; intimate familiarity with failure modes and system boundaries. Effective monitoring systems should help operators recognize the &ldquo;edge of the envelope&rdquo; - that threshold beyond which system behavior becomes unpredictable. This requires not just data collection but careful design of human-machine interfaces that make system state and trajectories legible to operators.</p>
<p>The fundamental insight is that safety isn&rsquo;t a product to be purchased or a feature to be coded - it&rsquo;s an emergent property arising from moment-to-moment adaptation by skilled human operators working with automated systems. Until we achieve true algorithmic omniscience (if ever), our focus must be on maintaining this creative tension between human and machine capabilities, keeping operators firmly within the loop while leveraging automation to extend rather than replace their capabilities.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tenzinwangdhen.com/tags/ai/">ai</a></li>
      <li><a href="https://tenzinwangdhen.com/tags/automation/">automation</a></li>
      <li><a href="https://tenzinwangdhen.com/tags/reliability/">reliability</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://tenzinwangdhen.com/posts/ai-replace-engineers/">
    <span class="title">« Prev</span>
    <br>
    <span>Will AI Displace Software Engineers?</span>
  </a>
  <a class="next" href="https://tenzinwangdhen.com/posts/nvidia-barbarians-at-the-moat/">
    <span class="title">Next »</span>
    <br>
    <span>Nvidia - Barbarians at the Moat</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on x"
            href="https://x.com/intent/tweet/?text=Catastrophic%20Failure%20in%20Complex%20Systems&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f&amp;hashtags=ai%2cautomation%2creliability">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f&amp;title=Catastrophic%20Failure%20in%20Complex%20Systems&amp;summary=Catastrophic%20Failure%20in%20Complex%20Systems&amp;source=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f&title=Catastrophic%20Failure%20in%20Complex%20Systems">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on whatsapp"
            href="https://api.whatsapp.com/send?text=Catastrophic%20Failure%20in%20Complex%20Systems%20-%20https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on telegram"
            href="https://telegram.me/share/url?text=Catastrophic%20Failure%20in%20Complex%20Systems&amp;url=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Catastrophic Failure in Complex Systems on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Catastrophic%20Failure%20in%20Complex%20Systems&u=https%3a%2f%2ftenzinwangdhen.com%2fposts%2fcatastrophic-failure-complex-systems%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://tenzinwangdhen.com">Tenzin&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
